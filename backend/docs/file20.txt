20
$7 \quad$ Another algorithm for maximizing $\ell(\theta)$
Returning to logistic regression with $g(z)$ being the sigmoid function, let's now talk about a different algorithm for maximizing $\ell(\theta)$.

To get us started, let's consider Newton's method for finding a zero of a function. Specifically, suppose we have some function $f: \mathbb{R} \mapsto \mathbb{R}$, and we wish to find a value of $\theta$ so that $f(\theta)=0$. Here, $\theta \in \mathbb{R}$ is a real number. Newton's method performs the following update:
\[
\theta:=\theta-\frac{f(\theta)}{f^{\prime}(\theta)} .
\]

This method has a natural interpretation in which we can think of it as approximating the function $f$ via a linear function that is tangent to $f$ at the current guess $\theta$, solving for where that linear function equals to zero, and letting the next guess for $\theta$ be where that linear function is zero.
Here's a picture of the Newton's method in action:
In the leftmost figure, we see the function $f$ plotted along with the line $y=0$. We're trying to find $\theta$ so that $f(\theta)=0$; the value of $\theta$ that achieves this is about 1.3. Suppose we initialized the algorithm with $\theta=4.5$. Newton's method then fits a straight line tangent to $f$ at $\theta=4.5$, and solves for the where that line evaluates to 0 . (Middle figure.) This give us the next guess for $\theta$, which is about 2.8. The rightmost figure shows the result of running one more iteration, which the updates $\theta$ to about 1.8. After a few more iterations, we rapidly approach $\theta=1.3$.

Newton's method gives a way of getting to $f(\theta)=0$. What if we want to use it to maximize some function $\ell$ ? The maxima of $\ell$ correspond to points where its first derivative $\ell^{\prime}(\theta)$ is zero. So, by letting $f(\theta)=\ell^{\prime}(\theta)$, we can use the same algorithm to maximize $\ell$, and we obtain update rule:
\[
\theta:=\theta-\frac{\ell^{\prime}(\theta)}{\ell^{\prime \prime}(\theta)} .
\]
(Something to think about: How would this change if we wanted to use Newton's method to minimize rather than maximize a function?)