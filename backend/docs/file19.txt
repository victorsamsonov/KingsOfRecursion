19

Above, we used the fact that $g^{\prime}(z)=g(z)(1-g(z))$. This therefore gives us the stochastic gradient ascent rule
\[
\theta_{j}:=\theta_{j}+\alpha\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}
\]

If we compare this to the LMS update rule, we see that it looks identical; but this is not the same algorithm, because $h_{\theta}\left(x^{(i)}\right)$ is now defined as a non-linear function of $\theta^{T} x^{(i)}$. Nonetheless, it's a little surprising that we end up with the same update rule for a rather different algorithm and learning problem. Is this coincidence, or is there a deeper reason behind this? We'll answer this when get get to GLM models. (See also the extra credit problem on Q3 of problem set 1.)
6 Digression: The perceptron learning algorithm

We now digress to talk briefly about an algorithm that's of some historical interest, and that we will also return to later when we talk about learning theory. Consider modifying the logistic regression method to "force" it to output values that are either 0 or 1 or exactly. To do so, it seems natural to change the definition of $g$ to be the threshold function:
\[
g(z)=\left\{\begin{array}{ll}
1 & \text { if } z \geq 0 \\
0 & \text { if } z<0
\end{array}\right.
\]

If we then let $h_{\theta}(x)=g\left(\theta^{T} x\right)$ as before but using this modified definition of $g$, and if we use the update rule
\[
\theta_{j}:=\theta_{j}+\alpha\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)} .
\]
then we have the perceptron learning algorithm.
In the 1960s, this "perceptron" was argued to be a rough model for how individual neurons in the brain work. Given how simple the algorithm is, it will also provide a starting point for our analysis when we talk about learning theory later in this class. Note however that even though the perceptron may be cosmetically similar to the other algorithms we talked about, it is actually a very different type of algorithm than logistic regression and least squares linear regression; in particular, it is difficult to endow the perceptron's predictions with meaningful probabilistic interpretations, or derive the perceptron as a maximum likelihood estimation algorithm.