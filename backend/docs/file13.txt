13
instead maximize the log likelihood $\ell(\theta)$ :
\[
\begin{aligned}
\ell(\theta) & =\log L(\theta) \\
& =\log \prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right) \\
& =\sum_{i=1}^{m} \log \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right) \\
& =m \log \frac{1}{\sqrt{2 \pi} \sigma}-\frac{1}{\sigma^{2}} \cdot \frac{1}{2} \sum_{i=1}^{m}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2} .
\end{aligned}
\]

Hence, maximizing $\ell(\theta)$ gives the same answer as minimizing
\[
\frac{1}{2} \sum_{i=1}^{m}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2},
\]
which we recognize to be $J(\theta)$, our original least-squares cost function.
To summarize: Under the previous probabilistic assumptions on the data, least-squares regression corresponds to finding the maximum likelihood estimate of $\theta$. This is thus one set of assumptions under which least-squares regression can be justified as a very natural method that's just doing maximum likelihood estimation. (Note however that the probabilistic assumptions are by no means necessary for least-squares to be a perfectly good and rational procedure, and there may - and indeed there are - other natural assumptions that can also be used to justify it.)

Note also that, in our previous discussion, our final choice of $\theta$ did not depend on what was $\sigma^{2}$, and indeed we'd have arrived at the same result even if $\sigma^{2}$ were unknown. We will use this fact again later, when we talk about the exponential family and generalized linear models.
4 Locally weighted linear regression
Consider the problem of predicting $y$ from $x \in \mathbb{R}$. The leftmost figure below shows the result of fitting a $y=\theta_{0}+\theta_{1} x$ to a dataset. We see that the data doesn't really lie on straight line, and so the fit is not very good.