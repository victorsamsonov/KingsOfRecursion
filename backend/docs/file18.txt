18

Let us assume that
\[
\begin{array}{l}
P(y=1 \mid x ; \theta)=h_{\theta}(x) \\
P(y=0 \mid x ; \theta)=1-h_{\theta}(x)
\end{array}
\]

Note that this can be written more compactly as
\[
p(y \mid x ; \theta)=\left(h_{\theta}(x)\right)^{y}\left(1-h_{\theta}(x)\right)^{1-y}
\]

Assuming that the $m$ training examples were generated independently, we can then write down the likelihood of the parameters as
\[
\begin{aligned}
L(\theta) & =p(\vec{y} \mid X ; \theta) \\
& =\prod_{i=1}^{m} p\left(y^{(i)} \mid x^{(i)} ; \theta\right) \\
& =\prod_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)\right)^{y^{(i)}}\left(1-h_{\theta}\left(x^{(i)}\right)\right)^{1-y^{(i)}}
\end{aligned}
\]

As before, it will be easier to maximize the log likelihood:
\[
\begin{aligned}
\ell(\theta) & =\log L(\theta) \\
& =\sum_{i=1}^{m} y^{(i)} \log h\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h\left(x^{(i)}\right)\right)
\end{aligned}
\]

How do we maximize the likelihood? Similar to our derivation in the case of linear regression, we can use gradient ascent. Written in vectorial notation, our updates will therefore be given by $\theta:=\theta+\alpha \nabla_{\theta} \ell(\theta)$. (Note the positive rather than negative sign in the update formula, since we're maximizing, rather than minimizing, a function now.) Let's start by working with just one training example $(x, y)$, and take derivatives to derive the stochastic gradient ascent rule:
\[
\begin{aligned}
\frac{\partial}{\partial \theta_{j}} \ell(\theta) & =\left(y \frac{1}{g\left(\theta^{T} x\right)}-(1-y) \frac{1}{1-g\left(\theta^{T} x\right)}\right) \frac{\partial}{\partial \theta_{j}} g\left(\theta^{T} x\right) \\
& =\left(y \frac{1}{g\left(\theta^{T} x\right)}-(1-y) \frac{1}{1-g\left(\theta^{T} x\right)}\right) g\left(\theta^{T} x\right)\left(1-g\left(\theta^{T} x\right) \frac{\partial}{\partial \theta_{j}} \theta^{T} x\right. \\
& =\left(y\left(1-g\left(\theta^{T} x\right)\right)-(1-y) g\left(\theta^{T} x\right)\right) x_{j} \\
& =\left(y-h_{\theta}(x)\right) x_{j}
\end{aligned}
\]