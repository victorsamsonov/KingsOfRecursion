{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database_from_docs_folder():\n",
    "\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.document_loaders import DirectoryLoader\n",
    "    import os\n",
    "\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "    #Load all the .txt files from docs directory\n",
    "    loader = DirectoryLoader('./docs/',glob = \"**/*.txt\")\n",
    "    documents = loader.load()\n",
    "    print('NUMBER OF DOCUMENTS LOADED: ', len(documents))\n",
    "\n",
    "    #Split text into tokens\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print('NUMBER OF CHUNKS: ', len(chunks))\n",
    "\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    print('Creating the embeddings...')\n",
    "    # embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    from langchain.vectorstores import Chroma\n",
    "\n",
    "    print('Creating the vector database...')\n",
    "    # db = Chroma.from_documents(chunks, embeddings, persist_directory=\"db\")\n",
    "    db = Chroma.from_documents(chunks, OpenAIEmbeddings())\n",
    "    print('Vector database created')\n",
    "    \n",
    "    return db\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_about_txts(user_query, db):\n",
    "\n",
    "    # user_query = \"What do they have to do with the survey?\"\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.chains import LLMChain\n",
    "    \n",
    "    k = 3\n",
    "    \n",
    "    print('Retrieving docs...')\n",
    "    # docs_retrieved = db.similarity_search(user_query, k=k)\n",
    "    docs_retrieved = db.max_marginal_relevance_search(user_query, k=k)\n",
    "\n",
    "    context = ''\n",
    "    \n",
    "    for i, doc_retrieved in enumerate(docs_retrieved):\n",
    "        context += '\\n'\n",
    "        context += f'\\n{i+1}. Most relevant document: '\n",
    "        context += '\\nPAGE CONTENT: ' + doc_retrieved.page_content\n",
    "        context += '\\nREFERENCE: ' + doc_retrieved.metadata['source']\n",
    "                \n",
    "    # context = docs_retrieved[0].page_content\n",
    "\n",
    "    template = \"\"\"You are gonna receive a <USER_QUERY>, and you should respond\n",
    "\n",
    "    based on the <RETRIEVED_DOCUMENTS>. \n",
    "\n",
    "    This is the <USER_QUERY>: {user_query}\n",
    "\n",
    "    These are the <RETRIEVED_DOCUMENTS>: {context} \n",
    "    \n",
    "    When you build your response, at the bottom of it, you should add the references \n",
    "    of the retrieved documents that you used. For instance, you should add all the\n",
    "    filenames of the REFERENCE values that you receive in <RETRIEVED_DOCUMENTS>.\n",
    "    \n",
    "    Example questions: What is the name of the professor?\n",
    "    \n",
    "    Response: \n",
    "    \n",
    "    The name of the professor is Yan Yan.\n",
    "    \n",
    "    This information was found in the next references: \n",
    "    \n",
    "    filename_1.txt\n",
    "    filename_2.txt\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"user_query\", \"context\"])\n",
    "\n",
    "    llm = OpenAI()\n",
    "\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    print('CONTEXT: ', context)\n",
    "    print('LEN CONTEXT: ', len(context))\n",
    "    print('METADATA: ', docs_retrieved[0].metadata)\n",
    "    \n",
    "    response = llm_chain.run({\n",
    "        'user_query': user_query,\n",
    "        'context': context\n",
    "    })\n",
    "    \n",
    "\n",
    "    print('RESPONSE: ', response)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF DOCUMENTS LOADED:  10\n",
      "NUMBER OF CHUNKS:  1744\n",
      "Creating the embeddings...\n",
      "Creating the vector database...\n",
      "Vector database created\n"
     ]
    }
   ],
   "source": [
    "db = create_database_from_docs_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving docs...\n",
      "CONTEXT:  \n",
      "\n",
      "1. Most relevant document: \n",
      "PAGE CONTENT: Thanks for being here for Lecture 5, uh, of CS230. Um, today we have, uh, the chance to, to host, uh, a guest speaker Pranav Rajpurkar, who is a PhD student, um, in computer science advised by, uh, Professor Andrew Ng and Professor Percy Liang. So Pranav is, uh,  is working on, um, AI and high impact projects, uh, specifically related to health care and natural language processing. And today he's going to, to present, um, an overview of AI for healthcare and he's going to dig into some projects\n",
      "REFERENCE: docs/IM9ANAbufYM.txt\n",
      "\n",
      "2. Most relevant document: \n",
      "PAGE CONTENT: with Professor Brunskill at Stanford. And he will tell us a little bit about his experience and he will show us some advanced applications of deep learning in RL, and how these plug in together. Thank you. Thanks Kian for that introduction. Okay. Can everyone hear me now? Right, good. Cool. Okay first, I have like, 8-9 minutes. You have more. I have more? Yes. Okay. Great, okay first question. After seeing that lecture so far like, how many are you-of you are thinking that RL is actually cool?\n",
      "REFERENCE: docs/NP2XqpgTJyo.txt\n",
      "\n",
      "3. Most relevant document: \n",
      "PAGE CONTENT: Includes TA sections and a next one- and every in-class lecture including next Wednesday. And this Friday you have a TA section. Any questions on that? Okay. See you next week, guys.\n",
      "REFERENCE: docs/IM9ANAbufYM.txt\n",
      "LEN CONTEXT:  1407\n",
      "METADATA:  {'source': 'docs/IM9ANAbufYM.txt'}\n",
      "RESPONSE:  \n",
      "    \n",
      "In response to your query, the name of the professor that is giving the lecture is Professor Andrew Ng. This information was found in the following references: docs/IM9ANAbufYM.txt, docs/NP2XqpgTJyo.txt.\n",
      "\n",
      "    \n",
      "In response to your query, the name of the professor that is giving the lecture is Professor Andrew Ng. This information was found in the following references: docs/IM9ANAbufYM.txt, docs/NP2XqpgTJyo.txt.\n"
     ]
    }
   ],
   "source": [
    "# user_query = 'What is the name of the CS230 course?'\n",
    "user_query = 'What is the name of the professor that is giving the lecture?'\n",
    "\n",
    "response = ask_question_about_txts(user_query, db)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: tiktoken: command not found\n"
     ]
    }
   ],
   "source": [
    "!tiktoken --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tiktoken' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(tiktoken\u001b[39m.\u001b[39;49m__version__)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(tiktoken\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tiktoken' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "print(tiktoken.__version__)\n",
    "print(tiktoken.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "try:\n",
    "    version = pkg_resources.get_distribution(\"tiktoken\").version\n",
    "    print(f\"tiktoken version: {version}\")\n",
    "except pkg_resources.DistributionNotFound:\n",
    "    print(\"tiktoken is not installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken is installed at: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "try:\n",
    "    tiktoken_distribution = pkg_resources.get_distribution(\"tiktoken\")\n",
    "    print(f\"tiktoken is installed at: {tiktoken_distribution.location}\")\n",
    "except pkg_resources.DistributionNotFound:\n",
    "    print(\"tiktoken is not installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
