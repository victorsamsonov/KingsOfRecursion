{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database_from_docs_folder():\n",
    "\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.document_loaders import DirectoryLoader\n",
    "    import os\n",
    "\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "    #Load all the .txt files from docs directory\n",
    "    loader = DirectoryLoader('./docs/',glob = \"**/*.txt\")\n",
    "    documents = loader.load()\n",
    "    print('NUMBER OF DOCUMENTS LOADED: ', len(documents))\n",
    "\n",
    "    #Split text into tokens\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print('NUMBER OF CHUNKS: ', len(chunks))\n",
    "\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    print('Creating the embeddings...')\n",
    "    # embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    from langchain.vectorstores import Chroma\n",
    "\n",
    "    print('Creating the vector database...')\n",
    "    # db = Chroma.from_documents(chunks, embeddings, persist_directory=\"db\")\n",
    "    db = Chroma.from_documents(chunks, OpenAIEmbeddings())\n",
    "    print('Vector database created')\n",
    "    \n",
    "    return db\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_about_txts(user_query, db):\n",
    "\n",
    "    # user_query = \"What do they have to do with the survey?\"\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.chains import LLMChain\n",
    "    \n",
    "    k = 3\n",
    "    \n",
    "    print('Retrieving docs...')\n",
    "    # docs_retrieved = db.similarity_search(user_query, k=k)\n",
    "    docs_retrieved = db.max_marginal_relevance_search(user_query, k=k)\n",
    "\n",
    "    context = ''\n",
    "    \n",
    "    for i, doc_retrieved in enumerate(docs_retrieved):\n",
    "        context += '\\n'\n",
    "        context += f'\\n{i+1}. Most relevant document: '\n",
    "        context += '\\nPAGE CONTENT: ' + doc_retrieved.page_content\n",
    "        context += '\\nREFERENCE: ' + doc_retrieved.metadata['source']\n",
    "                \n",
    "    # context = docs_retrieved[0].page_content\n",
    "\n",
    "    template = \"\"\"You are gonna receive a <USER_QUERY>, and you should respond\n",
    "\n",
    "    based on the <RETRIEVED_DOCUMENTS>. \n",
    "\n",
    "    This is the <USER_QUERY>: {user_query}\n",
    "\n",
    "    These are the <RETRIEVED_DOCUMENTS>: {context} \n",
    "    \n",
    "    When you build your response, at the bottom of it, you should add the references \n",
    "    of the retrieved documents that you used. For instance, you should add all the\n",
    "    filenames of the REFERENCE values that you receive in <RETRIEVED_DOCUMENTS>.\n",
    "    \n",
    "    Example questions: What is the name of the professor?\n",
    "    \n",
    "    Response: \n",
    "    \n",
    "    The name of the professor is Yan Yan.\n",
    "    \n",
    "    This information was found in the next references: \n",
    "    \n",
    "    filename_1.txt\n",
    "    filename_2.txt\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"user_query\", \"context\"])\n",
    "\n",
    "    llm = OpenAI()\n",
    "\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    print('CONTEXT: ', context)\n",
    "    print('LEN CONTEXT: ', len(context))\n",
    "    print('METADATA: ', docs_retrieved[0].metadata)\n",
    "    \n",
    "    response = llm_chain.run({\n",
    "        'user_query': user_query,\n",
    "        'context': context\n",
    "    })\n",
    "    \n",
    "\n",
    "    print('RESPONSE: ', response)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF DOCUMENTS LOADED:  10\n",
      "NUMBER OF CHUNKS:  1744\n",
      "Creating the embeddings...\n",
      "Creating the vector database...\n",
      "Vector database created\n"
     ]
    }
   ],
   "source": [
    "db = create_database_from_docs_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving docs...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# user_query = 'What is the name of the CS230 course?'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m user_query \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mWhat is the name of the professor that is giving the lecture?\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m response \u001b[39m=\u001b[39m ask_question_about_txts(user_query, db)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "\u001b[1;32m/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# context = docs_retrieved[0].page_content\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m template \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mYou are gonna receive a <USER_QUERY>, and you should respond\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mbased on the <RETRIEVED_DOCUMENTS>. \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate(template\u001b[39m=\u001b[39mtemplate, input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39muser_query\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m llm \u001b[39m=\u001b[39m OpenAI()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jferro/Desktop/work/repos/KingsOfRecursion/research_julen/langchain_txt.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m llm_chain \u001b[39m=\u001b[39m LLMChain(prompt\u001b[39m=\u001b[39mprompt, llm\u001b[39m=\u001b[39mllm)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "# user_query = 'What is the name of the CS230 course?'\n",
    "user_query = 'What is the name of the professor that is giving the lecture?'\n",
    "\n",
    "response = ask_question_about_txts(user_query, db)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
