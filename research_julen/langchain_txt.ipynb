{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database_from_docs_folder():\n",
    "\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.document_loaders import DirectoryLoader\n",
    "    import os\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.chains import LLMChain\n",
    "\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "    #Load all the .txt files from docs directory\n",
    "    loader = DirectoryLoader('./docs/',glob = \"**/*.txt\")\n",
    "    documents = loader.load()\n",
    "    print('NUMBER OF DOCUMENTS LOADED: ', len(documents))\n",
    "\n",
    "    #Split text into tokens\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print('NUMBER OF CHUNKS: ', len(chunks))\n",
    "\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    print('Creating the embeddings...')\n",
    "    # embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    from langchain.vectorstores import Chroma\n",
    "\n",
    "    print('Creating the vector database...')\n",
    "    # db = Chroma.from_documents(chunks, embeddings, persist_directory=\"db\")\n",
    "    db = Chroma.from_documents(chunks, OpenAIEmbeddings())\n",
    "    print('Vector database created')\n",
    "    \n",
    "    return db\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_about_txts(user_query, db):\n",
    "\n",
    "    # user_query = \"What do they have to do with the survey?\"\n",
    "    \n",
    "    k = 3\n",
    "    \n",
    "    print('Retrieving docs...')\n",
    "    # docs_retrieved = db.similarity_search(user_query, k=k)\n",
    "    docs_retrieved = db.max_marginal_relevance_search(user_query, k=k)\n",
    "\n",
    "    context = ''\n",
    "    \n",
    "    for i, doc_retrieved in enumerate(docs_retrieved):\n",
    "        context += '\\n'\n",
    "        context += f'\\n{i+1}. Most relevant document: '\n",
    "        context += '\\nPAGE CONTENT: ' + doc_retrieved.page_content\n",
    "        context += '\\nREFERENCE: ' + doc_retrieved.metadata['source']\n",
    "                \n",
    "    # context = docs_retrieved[0].page_content\n",
    "\n",
    "    template = \"\"\"You are gonna receive a <USER_QUERY>, and you should respond\n",
    "\n",
    "    based on the <RETRIEVED_DOCUMENTS>. \n",
    "\n",
    "    This is the <USER_QUERY>: {user_query}\n",
    "\n",
    "    These are the <RETRIEVED_DOCUMENTS>: {context} \n",
    "    \n",
    "    When you build your response, at the bottom of it, you should add the references \n",
    "    of the retrieved documents that you used. For instance, you should add all the\n",
    "    filenames of the REFERENCE values that you receive in <RETRIEVED_DOCUMENTS>.\n",
    "    \n",
    "    Example questions: What is the name of the professor?\n",
    "    \n",
    "    Response: \n",
    "    \n",
    "    The name of the professor is Yan Yan.\n",
    "    \n",
    "    This information was found in the next references: \n",
    "    \n",
    "    filename_1.txt\n",
    "    filename_2.txt\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"user_query\", \"context\"])\n",
    "\n",
    "    llm = OpenAI()\n",
    "\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    print('CONTEXT: ', context)\n",
    "    print('LEN CONTEXT: ', len(context))\n",
    "    print('METADATA: ', docs_retrieved[0].metadata)\n",
    "    \n",
    "    response = llm_chain.run({\n",
    "        'user_query': user_query,\n",
    "        'context': context\n",
    "    })\n",
    "    \n",
    "\n",
    "    print('RESPONSE: ', response)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF DOCUMENTS LOADED:  10\n",
      "NUMBER OF CHUNKS:  1744\n",
      "Creating the embeddings...\n",
      "Creating the vector database...\n",
      "Vector database created\n"
     ]
    }
   ],
   "source": [
    "db = create_database_from_docs_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving docs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT:  \n",
      "\n",
      "1. Most relevant document: \n",
      "PAGE CONTENT: Thanks for being here for Lecture 5, uh, of CS230. Um, today we have, uh, the chance to, to host, uh, a guest speaker Pranav Rajpurkar, who is a PhD student, um, in computer science advised by, uh, Professor Andrew Ng and Professor Percy Liang. So Pranav is, uh,  is working on, um, AI and high impact projects, uh, specifically related to health care and natural language processing. And today he's going to, to present, um, an overview of AI for healthcare and he's going to dig into some projects\n",
      "REFERENCE: docs/IM9ANAbufYM.txt\n",
      "\n",
      "2. Most relevant document: \n",
      "PAGE CONTENT: Includes TA sections and a next one- and every in-class lecture including next Wednesday. And this Friday you have a TA section. Any questions on that? Okay. See you next week, guys.\n",
      "REFERENCE: docs/IM9ANAbufYM.txt\n",
      "\n",
      "3. Most relevant document: \n",
      "PAGE CONTENT: at the end of this class. Nearly at the end this class. Um, you know, the NeurIPS conference is taking place right now.\n",
      "REFERENCE: docs/IFLstgCNOA4.txt\n",
      "LEN CONTEXT:  1028\n",
      "METADATA:  {'source': 'docs/IM9ANAbufYM.txt'}\n",
      "RESPONSE:  \n",
      "In response to the user query, the professor giving the lecture is Professor Andrew Ng and Professor Percy Liang. This information was found in the following references: docs/IM9ANAbufYM.txt.\n",
      "\n",
      "In response to the user query, the professor giving the lecture is Professor Andrew Ng and Professor Percy Liang. This information was found in the following references: docs/IM9ANAbufYM.txt.\n"
     ]
    }
   ],
   "source": [
    "# user_query = 'What is the name of the CS230 course?'\n",
    "user_query = 'What is the name of the professor that is giving the lecture?'\n",
    "\n",
    "response = ask_question_about_txts(user_query, db)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
